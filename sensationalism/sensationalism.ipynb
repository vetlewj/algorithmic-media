{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "To get this work ignore the infer method. Copy the model data to the temporary storage into a newly created sensationalism folder."
   ],
   "metadata": {
    "id": "G73lie38tZOm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n"
   ],
   "metadata": {
    "id": "7bFbiRUgm-b3",
    "ExecuteTime": {
     "end_time": "2024-10-31T04:12:09.599886Z",
     "start_time": "2024-10-31T04:12:09.595675Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "def enforce_reproducibility(seed=1000):\n",
    "    # Sets seed manually for both CPU and CUDA\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # For atomic operations there is currently\n",
    "    # no simple way to enforce determinism, as\n",
    "    # the order of parallel operations is not known.\n",
    "    # CUDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # System based\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ],
   "metadata": {
    "id": "sL5lORLDn4Lz",
    "ExecuteTime": {
     "end_time": "2024-10-31T04:11:59.346204Z",
     "start_time": "2024-10-31T04:11:59.339424Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA\")\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "seed = 100\n",
    "enforce_reproducibility(seed)\n",
    "model = \"/Users/julianstrietzel/UCB/algorithmic-media/sensationalism/models/sensationalism/1000\"\n",
    "\n",
    "tk = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipe = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tk,\n",
    "    device=device,\n",
    "    task=\"text-classification\",\n",
    "    # function_to_apply='none' if args.task == 'sensationalism' else 'softmax', # Regression\n",
    "    truncation=\"longest_first\",\n",
    "    max_length=512,\n",
    ")"
   ],
   "metadata": {
    "id": "eVYTvZSrnwtB",
    "ExecuteTime": {
     "end_time": "2024-10-31T04:14:45.961255Z",
     "start_time": "2024-10-31T04:14:45.350663Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "x = pipe(\"This is a test sentence.\")\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5aocSuVzvGiR",
    "outputId": "1cdb4534-2fd2-42c7-a8d5-3c7e97d7ed61",
    "ExecuteTime": {
     "end_time": "2024-10-31T04:14:47.050559Z",
     "start_time": "2024-10-31T04:14:46.925933Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T04:14:48.051485Z",
     "start_time": "2024-10-31T04:14:48.039164Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.43523022532463074}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
