---
output:
pdf_document: default
html_document: default
---

[Just as a quick reminder on how to interpret logarithmic regression models: \
**Rules for interpretation**](https://library.virginia.edu/data/articles/interpreting-log-transformations-in-a-linear-model#:~:text=Rules%20for%20interpretation)

OK, you ran a regression/fit a linear model and some of your variables are log-transformed.

-   **Only the dependent/response variable is log-transformed**. Exponentiate the coefficient. This gives the **multiplicative factor for every one-unit increase in the independent variable**. \
Example: the coefficient is 0.198. exp(0.198) = 1.218962. For every one-unit increase in the independent variable, our dependent variable increases by a factor of about 1.22, or 22%. Recall that multiplying a number by 1.22 is the same as increasing the number by 22%. Likewise, multiplying a number by, say 0.84, is the same as decreasing the number by 1 – 0.84 = 0.16, or 16%.\
-\> Notably for most small coefficients (like 0.024) this is directly 2.4% increase as exp(0.024) \~= 1.024

-   **Both dependent/response variable and independent/predictor variable(s) are log-transformed**. Interpret the coefficient as **the percent increase in the dependent variable for every 1% increase in the independent variable**. Example: the coefficient is 0.198. For every 1% increase in the independent variable, our dependent variable increases by about 0.20%. For x percent increase, calculate 1.x to the power of the coefficient, subtract 1, and multiply by 100. Example: For every 20% increase in the independent variable, our dependent variable increases by about (1.20 ^0.198^ - 1) \* 100 = 3.7 percent.\
-\> Already in percentage if double log!

```{r, include=FALSE}
library(data.table)
library(ggplot2)
library(here)
library(dplyr)
library(progress)
library(stargazer)
library(stringr)
library(broom)
```

```{r, include=TRUE}
data <- fread(here("data", "combined_for_analysis_sensationalism_jargon_categories_domain_labels.csv"))
# rename Engineering & Tech to Engineering and Tech
data[, top_category := gsub("Engineering & Tech", "Engineering and Tech", top_category)]
# drop column V1
data <- data[, -1]
sum(is.na(data))
data = na.omit(data)
colnames(data)
```

```{r, include=TRUE}
model_jargon_only = lm(score ~ jargon_proportion, data = data)
model_year = lm(score ~ jargon_proportion + factor(year), data = data)
model_year_month = lm(score ~ jargon_proportion + factor(year) + factor(month), data = data)
model_year_month_top_cat = lm(score ~ jargon_proportion +
  factor(year) +
  factor(month) +
  factor(top_category), data = data)
stargazer(model_jargon_only, model_year, model_year_month, model_year_month_top_cat,
          type = "text",
          omit = "factor",
          column.labels = c("Jargon only", "Jargon, Year",
                            "Jargon, Year and Month", "Jargon, Year, Month, Category")
)
```

-\> We can explain a bit more, when including factors for year months etc. Let's have a look whether that is significantly different:

```{r, include=TRUE}
anova(model_jargon_only, model_year)
anova(model_year, model_year_month)
anova(model_year_month, model_year_month_top_cat)
```

*-\> Seems like every single step: Adding year, month and top_category adds power to the model. :::*

Let's look how the months influence our score?

```{r, include=TRUE}
stargazer(model_year, model_year_month,
                     type = "text",
                     omit = "year"
)
```

-\> For plotting this look at python analysis file ::: I feel like posts from December are just the best adding about an extra 250 points to the score. ::: Let's see how the year interacts with the jargon on the score:

```{r, include=TRUE}
# Interaction terms in years and jargon
model_interaction_jargon_year = lm(score ~ jargon_proportion:factor(year) +
  factor(year) +
  factor(month) - 1, data = data)
anova(model_interaction_jargon_year, model_year_month)

stargazer(model_interaction_jargon_year,
          type = "text",
          omit = 1:18
)

```

Okay seems like that works. Let's look at the interaction terms. I interpret this as how effective the jargon is in the different years.

But: Look at anova: Adding those interactions doesn't really make a difference, we should ignore it.

*Adding interactions between year and jargon doesn't really make a difference, we should ignore it.*

We could do something similar for the month, but I am not sure if it is worth it. Let's do it for the **categories**.

```{r, include=TRUE}
model_interaction_jargon_top_category_log = lm(log(score + 1) ~
                                                 jargon_proportion:factor(top_category)
                                                   +
                                                   factor(year)
                                                   +
                                                   factor(month)
                                                   +
                                                   factor(top_category),
                                               data = data
)
model_interaction_jargon_top_category = lm(score ~
                                             jargon_proportion:factor(top_category)
                                               +
                                               factor(year)
                                               +
                                               factor(month)
                                               +
                                               factor(top_category),
                                           data = data
)
model_no_interaction_jargon_top_cat = lm(log(score + 1) ~
                                           jargon_proportion
                                             +
                                             factor(year)
                                             +
                                             factor(month)
                                             +
                                             factor(top_category),
                                         data = data
)
anova(model_interaction_jargon_top_category, model_no_interaction_jargon_top_cat)
stargazer(model_interaction_jargon_top_category_log, model_no_interaction_jargon_top_cat, type = "text", omit = c("year", "month"),
          covariate.labels = c("Jargon Proportion",
                               "Environmental Sciences", "Life Sciences", "Physical Sciences", "Social Sciences",
                               "Jargon x Engineering and Tech", "Jargon x Environmental Sciences", "Jargon x Life Sciences", "Jargon x Physical Sciences", "Jargon x Social Sciences")
)
```

```{r, include=TRUE}

model_interaction_link_flair = data[, lm(score ~ jargon_proportion:factor(link_flair_text) +
  factor(year) +
  factor(month) +
  factor(top_category))]
model_interaction_top_cat = data[, lm(score ~ jargon_proportion:factor(top_category) +
  factor(year) +
  factor(month) +
  factor(top_category))]
model_interaction_link_flair_log = data[, lm(log(score + 1) ~ jargon_proportion:factor(link_flair_text) +
  factor(year) +
  factor(month) +
  factor(top_category))]
model_interaction_top_cat_log = data[, lm(log(score + 1) ~ jargon_proportion:factor(top_category) +
  factor(year) +
  factor(month) +
  factor(top_category))]
stargazer(model_interaction_link_flair, model_interaction_top_cat, type = "text", omit = 1:17)
```

```{r, include=TRUE}
stargazer(model_interaction_link_flair_log, model_interaction_top_cat_log, type = "text", omit = 1:17)
```

-   Interaction terms between jargon and the top categories do seem to be interpretable and significant but not that more explanatory\*

-\> Go with top_category only, as it is more interpretable and readable and adjusted r2 is similar in linear regression ON the other hand if using log score, the interaction terms are more significant and explanatory when including everything!

## Trying to make it more interpretable

```{r, include=TRUE}
data[, jargon_percentage_times_100 := (jargon_proportion) * 100]
model_interaction_no_log = lm(score ~ jargon_percentage_times_100:factor(top_category) +
  factor(year) +
  factor(month) +
  factor(top_category), data = data)
model_log_100_times = lm(log(score + 1) ~ jargon_percentage_times_100:factor(top_category) +
  factor(year) +
  factor(month) +
  factor(top_category), data = data)
model_log_log = lm(log(score + 1) ~ log(jargon_proportion + 0.00001):factor(top_category) +
  factor(year) +
  factor(month) +
  factor(top_category), data = data)


stargazer(model_interaction_no_log, model_log_100_times, model_log_log, type = "text", omit = c(1:17), covariate.labels = c("Evnironmental Sciences", "Life Sciences", "Physical Sciences", "Social Sciences",
                               "Jargon x 100 x Engineering and Tech", "Jargon x 100 x Environmental Sciences", "Jargon x 100 x Life Sciences", "Jargon x 100 x Physical Sciences", "Jargon x 100 x Social Sciences",
                               "Log Jargon x Engineering and Tech", "Log Jargon x Environmental Sciences", "Log Jargon x Life Sciences", "Log Jargon x Physical Sciences", "Log Jargon x Social Sciences"))
```

This table presents regression results that explain variations in `score`, `log(score + 1)`, and an alternative log-transformed score measure (in columns (1), (2), and (3) respectively), based on different scientific disciplines and the interaction effects of jargon with each discipline. Let’s interpret the coefficients for each of these categories, focusing on their percentage change implications where relevant.

### Column (1): `score`

This column examines the raw `score` as the dependent variable. The coefficients here represent absolute changes in `score` associated with each scientific discipline or the influence of jargon on the score.

1.  **Environmental Sciences (590.998)**
-   **Interpretation**: Articles in the `Environmental Sciences` category are associated with a substantial increase of about **591 points** in score compared to the baseline category, indicating significantly higher engagement.
2.  **Life Sciences (531.648)**
-   **Interpretation**: Articles in the `Life Sciences` category are associated with an increase of about **532 points** in score, also suggesting strong engagement, though slightly lower than `Environmental Sciences`.
3.  **Physical Sciences (-49.573)**
-   **Interpretation**: Articles in `Physical Sciences` are associated with a decrease of approximately **50 points** in score, indicating lower engagement compared to the baseline.
4.  **Social Sciences (1,326.293)**
-   **Interpretation**: `Social Sciences` articles are associated with the largest increase, about **1,326 points** in score, making them the most engaging among these disciplines.

#### Jargon x 100 Interactions

The interaction terms of `Jargon x 100` with each discipline indicate how a 1% increase in jargon affects the score within each specific discipline.

1.  **Jargon x 100 x Engineering and Tech (3.985)**
-   **Interpretation**: In `Engineering and Technology`, each 1% increase in jargon correlates with about a **4-point increase** in score, but this effect is not statistically significant.
2.  **Jargon x 100 x Environmental Sciences (6.811)**
-   **Interpretation**: In `Environmental Sciences`, each 1% increase in jargon correlates with about a **6.8-point increase** in score, significant at the 0.1 level. This suggests a moderate positive impact of jargon on engagement.
3.  **Jargon x 100 x Life Sciences (4.116)**
-   **Interpretation**: In `Life Sciences`, each 1% increase in jargon correlates with an approximate **4-point increase** in score, significant at the 1% level, showing that jargon slightly enhances engagement in this category.
4.  **Jargon x 100 x Physical Sciences (4.265)**
-   **Interpretation**: In `Physical Sciences`, each 1% increase in jargon correlates with a **4.3-point increase** in score, significant at the 0.1 level, indicating a small positive effect of jargon.
5.  **Jargon x 100 x Social Sciences (28.578)**
-   **Interpretation**: In `Social Sciences`, each 1% increase in jargon results in a substantial **28.6-point increase** in score, indicating a strong positive impact of jargon on engagement in this field.

**Summary**: In terms of raw scores, `Social Sciences` articles are the most engaging, followed by `Environmental Sciences` and `Life Sciences`. The impact of jargon is generally positive across disciplines, with the strongest effect in `Social Sciences`.

### Column (2): `log(score + 1)`

This column uses a log-transformed version of the score, so the coefficients can be interpreted as percentage changes in the score.

1.  **Environmental Sciences (1.312)**
-   **Interpretation**: Articles in `Environmental Sciences` are associated with an approximate **271% increase** in score compared to the baseline (calculated as $e^{1.312} - 1$), indicating high engagement.
2.  **Life Sciences (0.888)**
-   **Interpretation**: Articles in `Life Sciences` are associated with a **143.1% increase** in score, showing strong engagement.
3.  **Physical Sciences (0.413)**
-   **Interpretation**: Articles in `Physical Sciences` are associated with a **51.1% increase** in score, though this is lower than the other disciplines.
4.  **Social Sciences (1.206)**
-   **Interpretation**: Articles in `Social Sciences` show a **234.2% increase** in score, also indicating high engagement.

#### Jargon x 100 Interactions

These interaction terms show how a 1% (percentage points, meaning e.g. from 20 to 21 in jargon proportion) increase in jargon impacts the engagement score within each discipline.

1.  **Jargon x 100 x Engineering and Tech (0.025)**
-   **Interpretation**: In `Engineering and Technology`, each 1% increase in jargon is associated with a **2.5% increase** in score, suggesting a small positive impact of jargon.
2.  **Jargon x 100 x Environmental Sciences (0.015)**
-   **Interpretation**: In `Environmental Sciences`, each 1% increase in jargon results in a **1.5% increase** in score, indicating a mild positive effect.
3.  **Jargon x 100 x Life Sciences (0.021)**
-   **Interpretation**: In `Life Sciences`, each 1% increase in jargon results in a **2.1% increase** in score, showing a small positive impact of jargon.
4.  **Jargon x 100 x Physical Sciences (0.024)**
-   **Interpretation**: In `Physical Sciences`, each 1% increase in jargon leads to a **2.4% increase** in score, suggesting a modest positive effect of jargon.
5.  **Jargon x 100 x Social Sciences (0.032)**
-   **Interpretation**: In `Social Sciences`, each 1% increase in jargon results in a **3.2% increase** in score, indicating a stronger positive effect of jargon.

**Summary**: In this log-transformed model, `Environmental Sciences` and `Social Sciences` show the highest percentage increases in engagement. The interaction terms indicate that jargon positively affects engagement across disciplines, with the strongest effect in `Social Sciences`.

### Column (3): Alternative Log Score Transformation (Interpretation Similar to Column (2))

1.  **Environmental Sciences (1.011)**
-   **Interpretation**: Articles in `Environmental Sciences` are associated with a **174.7% increase** in score, indicating high engagement.
2.  **Life Sciences (0.776)**
-   **Interpretation**: Articles in `Life Sciences` show a **117.3% increase** in score, suggesting strong engagement.
3.  **Physical Sciences (0.303)**
-   **Interpretation**: Articles in `Physical Sciences` are associated with a **35.4% increase** in score, a more modest level of engagement.
4.  **Social Sciences (1.375)**
-   **Interpretation**: Articles in `Social Sciences` exhibit a **294.8% increase** in score, indicating the highest engagement in this model.

#### Log Jargon Interactions

These terms capture how logarithmic increases in jargon affect score within each discipline.

1.  **Log Jargon x Engineering and Tech (0.122)**
-   **Interpretation**: A 1% increase in jargon in `Engineering and Technology` is associated with a **1.22% increase** in score, showing a small positive effect.
2.  **Log Jargon x Environmental Sciences (0.120)**
-   **Interpretation**: In `Environmental Sciences`, a 10% increase in jargon correlates with a **1.20% increase** in score, showing a small positive impact.
3.  **Log Jargon x Life Sciences (0.131)**
-   **Interpretation**: In `Life Sciences`, a 10% increase in jargon correlates with a **1.31% increase** in score, suggesting a small positive effect.
4.  **Log Jargon x Physical Sciences (0.111)**
-   **Interpretation**: In `Physical Sciences`, a 10% increase in jargon is associated with a **1.11% increase** in score, indicating a mild positive effect.
5.  **Log Jargon x Social Sciences (0.186)**
-   **Interpretation**: In `Social Sciences`, a 10% increase in jargon correlates with a **1.86% increase** in score, showing the strongest positive effect.

**Summary**: Across the models, `Social Sciences` consistently have the highest engagement scores, followed by `Environmental Sciences` and `Life Sciences`. In all columns, jargon has a positive impact on engagement, particularly in `Social Sciences`. This implies that technical language may enhance engagement, especially in disciplines where readers expect or value jargon.

This is a good one: It shows how we decided to scale the jargon and how it influences the score. The log-log model is the most interpretable and super explenatory.

```{r, include=TRUE}
model_interaction_no_log_all_cat = lm(score ~ jargon_percentage_times_100:factor(link_flair_text) +
  factor(year) +
  factor(month) +
  factor(link_flair_text), data = data)
model_log_100_times_all_cat = lm(log(score + 1) ~ jargon_percentage_times_100:factor(link_flair_text) +
  factor(year) +
  factor(month) +
  factor(link_flair_text), data = data)
model_log_100_times_all_cat_top = lm(log(score + 1) ~ jargon_percentage_times_100:factor(link_flair_text) +
  factor(year) +
  factor(month) +
  factor(top_category), data = data)
model_log_log_all_cat = lm(log(score + 1) ~ log(jargon_proportion + 0.00001):factor(link_flair_text) +
  factor(year) +
  factor(month) +
  factor(link_flair_text), data = data)
model_log_log_all_cat_top = lm(log(score + 1) ~ log(jargon_proportion + 0.00001):factor(link_flair_text) +
  factor(year) +
  factor(month) +
  factor(top_category), data = data)
stargazer(model_log_100_times_all_cat_top, model_log_log_all_cat_top, type = "text", omit = 1:21, covariate.labels = c(
  "Jargon * Animal Science",
  "Jargon * Anthropology",
  "Jargon * Astronomy",
  "Jargon * Biology",
  "Jargon * Cancer",
  "Jargon * Chemistry",
  "Jargon * Computer Science",
  "Jargon * Earth Science",
  "Jargon * Economics",
  "Jargon * Engineering",
  "Jargon * Environment",
  "Jargon * Epidemiology",
  "Jargon * Genetics",
  "Jargon * Geology",
  "Jargon * Health",
  "Jargon * Materials Science",
  "Jargon * Mathematics",
  "Jargon * Medicine",
  "Jargon * Nanoscience",
  "Jargon * Neuroscience",
  "Jargon * Paleontology",
  "Jargon * Physics",
  "Jargon * Psychology",
  "Jargon * Social Science",
  "Log(Jargon) * Animal Science",
  "Log(Jargon) * Anthropology",
  "Log(Jargon) * Astronomy",
  "Log(Jargon) * Biology",
  "Log(Jargon) * Cancer",
  "Log(Jargon) * Chemistry",
  "Log(Jargon) * Computer Science",
  "Log(Jargon) * Earth Science",
  "Log(Jargon) * Economics",
  "Log(Jargon) * Engineering",
  "Log(Jargon) * Environment",
  "Log(Jargon) * Epidemiology",
  "Log(Jargon) * Genetics",
  "Log(Jargon) * Geology",
  "Log(Jargon) * Health",
  "Log(Jargon) * Materials Science",
  "Log(Jargon) * Mathematics",
  "Log(Jargon) * Medicine",
  "Log(Jargon) * Nanoscience",
  "Log(Jargon) * Neuroscience",
  "Log(Jargon) * Paleontology",
  "Log(Jargon) * Physics",
  "Log(Jargon) * Psychology",
  "Log(Jargon) * Social Science"
))
#stargazer(model_log_100_times_all_cat, model_log_log_all_cat, type = "text", omit=1:17)
```

Exemplary interpretation:

*semi-log* but jargon scaled from 0-100: Social Sciences - A 1-percentage-point increase in jargon_proportion results in an approximate 3.2% increase in score.

*log-log*: Social Sciences - A 1% increase in jargon_proportion results in an approximate .186% increase in score.

```{r, include=TRUE}
model_jargon_domain = lm(jargon_proportion ~
                           #factor(year) +
                           #factor(month) +
                           factor(label_voting_manual)
  , data = data)
model_score_domain = lm(score ~
                          #factor(year) +
                          #factor(month) +
                          factor(label_voting_manual)
  , data = data)
model_jargon_domain_log = lm(log(jargon_proportion + 0.00001) ~
                               factor(label_voting_manual)
  , data = data)
model_score_domain_log = lm(log(score + 1) ~
                              factor(label_voting_manual)
  , data = data)
#stargazer(model_jargon_domain, model_score_domain, type = "text")
stargazer(model_jargon_domain_log, model_score_domain_log, type = "text")

```

-\> You can explain 11 % of the variance in the jargon_proportion by the domain labels. The domain labels explain 12% of the variance in the score. The log model is more interpretable and explanatory.

Interpretation: e\^ coefficient - 1 x 100 = % change in jargon_proportion / score To expand the interpretation across the key categories (`scam`, `social_media`, `news`, `scientific`, and `repo`) in both columns, we can calculate the percentage change in `jargon_proportion` and `score` associated with each category compared to the reference category. Here’s how each of these categories relates to `jargon_proportion` and `score`, expressed as percentage changes.

### Column (1): `log(jargon_proportion + 1e-05)`

This column shows how each category affects the jargon proportion. The interpretation below reflects changes in jargon proportion compared to the reference category.

1.  **`scam` (-2.773)**: $$
\text{Percentage change} = (e^{-2.773} - 1) \times 100\% \approx -93.9\%
$$ **Interpretation**: `Scam` articles use significantly less jargon, with an approximate **93.9% decrease** in jargon proportion compared to the reference category.

2.  **`social_media` (-3.607)**: $$
\text{Percentage change} = (e^{-3.607} - 1) \times 100\% \approx -97.3\%
$$ **Interpretation**: `Social media` posts also use much less jargon, showing an approximate **97.3% decrease** in jargon proportion compared to the reference.

3.  **`news` (0.059)**: $$
\text{Percentage change} = (e^{0.059} - 1) \times 100\% \approx +6.1\%
$$ **Interpretation**: `News` articles have a slightly higher jargon proportion, with an approximate **6.1% increase** in jargon compared to the reference, though this increase is not statistically significant at conventional levels (e.g., p \< 0.05).

4.  **`scientific` (1.115)**: $$
\text{Percentage change} = (e^{1.115} - 1) \times 100\% \approx +205.1\%
$$ **Interpretation**: `Scientific` articles, as expected, use more jargon, with an approximate **205.1% increase** in jargon proportion compared to the reference. This significant increase likely reflects the technical nature of scientific content.

5.  **`repo` (0.095)**: $$
\text{Percentage change} = (e^{0.095} - 1) \times 100\% \approx +10.0\%
$$ **Interpretation**: `Repo` articles (likely referring to articles published on scientific repositories) show a **10.0% increase** in jargon compared to the reference, though this increase is modest and not statistically significant.

**Summary**: - **`Scam` and `social_media`** categories have significantly lower jargon levels, suggesting that content in these categories is generally more accessible or written in simpler language. - **`Scientific` and `repo`** categories are associated with higher jargon, with `scientific` articles showing a large increase, indicating their complex and technical nature. - **`News`** articles show a slight increase in jargon, but this effect is modest and not statistically robust.

### Column (2): `log(score + 1)`

This column reflects how each category impacts the engagement score (measured as log-transformed score). The interpretations below represent changes in score compared to the reference category.

1.  **`scam` (-0.486)**: $$
\text{Percentage change} = (e^{-0.486} - 1) \times 100\% \approx -38.5\%
$$ **Interpretation**: `Scam` articles are associated with an approximate **38.5% decrease** in engagement score compared to the reference, suggesting lower user engagement.

2.  **`social_media` (-0.794)**: $$
\text{Percentage change} = (e^{-0.794} - 1) \times 100\% \approx -55.7\%
$$ **Interpretation**: `Social media` posts also show a decrease in engagement, with an approximate **55.7% lower score** than the reference. This decrease suggests that content in this category receives less engagement on average.

3.  **`news` (1.680)**: $$
\text{Percentage change} = (e^{1.680} - 1) \times 100\% \approx +438.1\%
$$ **Interpretation**: `News` articles are associated with a substantial **438.1% increase** in engagement score, indicating that they are highly engaging and tend to attract more interactions compared to the reference.

4.  **`scientific` (1.551)**: $$
\text{Percentage change} = (e^{1.551} - 1) \times 100\% \approx +368.8\%
$$ **Interpretation**: `Scientific` articles also see a significant **368.8% increase** in engagement score, suggesting they are highly engaging, likely due to interest in scientific discoveries or discussions.

5.  **`repo` (1.087)**: $$
\text{Percentage change} = (e^{1.087} - 1) \times 100\% \approx +196.6\%
$$ **Interpretation**: `Repo` articles have an approximate **196.6% increase** in engagement score compared to the reference. This suggests that articles published on repositories are relatively engaging, possibly because they often feature new research findings.

**Summary**: - **`News`, `scientific`, and `repo`** articles show large increases in engagement, with `news` articles having the highest boost in score. This indicates that these categories are popular and likely to receive more upvotes and comments. - **`Scam` and `social_media`** categories, in contrast, see significant decreases in engagement, suggesting that users may be less interested in these types of content.

### Overall Interpretation

-   **Jargon Proportion**: Categories associated with scientific or technical content (e.g., `scientific` and `repo`) have higher jargon levels, while more accessible or general-interest categories (`scam` and `social_media`) have lower jargon proportions.
-   **Engagement (Score)**: `News`, `scientific`, and `repo` articles tend to receive more engagement, indicating that users find these topics more engaging. `Scam` and `social_media` articles tend to receive less engagement, suggesting they are less appealing or trusted by users.

These findings align with typical audience interests on science-related forums, where scientific discoveries, research, and news are more engaging than less credible or informal sources.

# Interaction jargon to log score within domains
```{r}
model_interaction_jargon_score_domains = lm(log(score + 1) ~ jargon_percentage_times_100:factor(label_voting_manual) +
  factor(year) +
  factor(month) +
  factor(label_voting_manual), data = data)
stargazer(model_interaction_jargon_score_domains, type = "text", omit = 1:18)
```
repo: 9.05212919314
scientific: 18.1923286049
news: 15.4405937628
social_media: 1.8202975923
scam: 1.9620699247
